{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8262715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geospatial\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "\n",
    "# base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, itertools, time\n",
    "#from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# iteration prints\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dbde40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elev',\n",
       " 'slope',\n",
       " 'elev_cv',\n",
       " 'loreys_height',\n",
       " 'percentage_first_returns_above_2m',\n",
       " 'total_biomass',\n",
       " 'vlce',\n",
       " 'Change_Attribution']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folder locations\n",
    "scratch = os.path.join(\"D:\\\\\", \"scratch\")\n",
    "shapefiles = os.path.join(\"..\", \"data\", \"shapefiles\")\n",
    "\n",
    "merged = os.path.join(\"D:\\\\\", \"Merged\")\n",
    "h_mask_loc = os.path.join(merged, \"harvest_mask\", \"harvest_mask.tif\")\n",
    "masked_structure_locs = os.path.join(merged, \"masked_structure\")\n",
    "bec_mask_loc = os.path.join(merged, \"bec_masks\")\n",
    "\n",
    "# variables\n",
    "structure_vars = os.listdir(masked_structure_locs)\n",
    "elev_vars = [\"elev\", \"slope\"]\n",
    "all_vars  = elev_vars + structure_vars + [\"vlce\", \"Change_Attribution\"]\n",
    "all_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac5c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_key = pd.read_csv(os.path.join(\"..\", \"keys\", \"continuous.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269da7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppa_bec = gpd.read_file(os.path.join(shapefiles, \"bc_ppa_bec_hres.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24634fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some generic cleaning of the gdf\n",
    "# calculate area for each polygon\n",
    "ppa_bec[\"Shap_Ar\"] = ppa_bec.area\n",
    "\n",
    "# assign a protected column to the df\n",
    "# this lambda thing is like a local function apparently? im not entirely sure whats going on\n",
    "# but it works\n",
    "ppa_bec = ppa_bec.assign(protected = lambda dataframe: dataframe[\"NAME_E\"].map(lambda NAME_E: True if NAME_E else False))\n",
    "\n",
    "# create a unique value for zone/subzones\n",
    "ppa_bec[\"szs\"] = ppa_bec[\"zone\"] + \"_\" + ppa_bec[\"subzone\"]\n",
    "\n",
    "# filter out any without bec zones (generally sliver polygons)\n",
    "ppa_bec = ppa_bec[ppa_bec[\"szs\"].notnull()]\n",
    "\n",
    "ppa_bec = ppa_bec[[\"szs\", \"geometry\", \"zone\", \"subzone\", \"protected\", \"Shap_Ar\"]]\n",
    "ppa_bec[\"raster\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4714dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subzones = sorted(list(set(ppa_bec[\"szs\"])))\n",
    "subzones.remove(\"SBPS_mk\") # doesnt have a protected area, can't compare\n",
    "# but useful for some other stuff so processed indiviudally later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3af2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rasterize_polygon_mask(gpd, out_loc, meta):\n",
    "    # rasterizes a polygon. requires a field in gpd named 'raster'\n",
    "    # masks out harvested pixels. requires\n",
    "    \n",
    "    with rasterio.open(out_loc, \"w+\", **meta) as out:\n",
    "        out_arr = out.read(1)\n",
    "\n",
    "        shapes = ((geom,value) for geom, value in zip(gpd.geometry, gpd.raster))\n",
    "\n",
    "        burned = features.rasterize(shapes = shapes, fill = 0, out = out_arr, transform = out.transform)\n",
    "\n",
    "        # remove this to prevent harvest from being masked out\n",
    "        # burned = burned * h_mask_data\n",
    "        \n",
    "        out.write_band(1, burned)\n",
    "        \n",
    "        return burned\n",
    "\n",
    "def sample_raster(mask, save_loc, big, small):\n",
    "    \n",
    "    sampled_loc = save_loc[:-4] + \"-sampled\" + save_loc[-4:]\n",
    "    \n",
    "    sampled_loc = os.path.split(sampled_loc)[1]\n",
    "    \n",
    "    sampled_loc = os.path.join(bec_mask_loc, sampled_loc)\n",
    "    \n",
    "    \n",
    "    # find valid indexes using mask, then randomly select from them until\n",
    "    # an equal number to the smaller (p/np) proportion of the bec zone\n",
    "    # is found\n",
    "    indexes = np.where(mask == 1)\n",
    "    good_indexes = np.random.choice(np.arange(0, big), size = small, replace = False)\n",
    "    \n",
    "    x = indexes[0][good_indexes]\n",
    "    y = indexes[1][good_indexes]\n",
    "\n",
    "    # generate blank raster, and make new mask based on this by taking\n",
    "    # xy coords and changing them to 1\n",
    "    blank = np.zeros(mask.shape, dtype = \"uint8\")\n",
    "    blank[x, y] = 1\n",
    "\n",
    "    # save with the sampled suffix, so it can be used without \n",
    "    # reprocessing\n",
    "    with rasterio.open(sampled_loc, \"w+\", **meta) as out:\n",
    "\n",
    "        out.write_band(1, blank)\n",
    "    \n",
    "    return blank\n",
    "\n",
    "def equal_sample_p_np(soi):\n",
    "    np.random.seed(69420)\n",
    "    \n",
    "    # filter ppa for the subzone of interest\n",
    "    subzone_filter = ppa_bec[ppa_bec[\"szs\"] == soi]\n",
    "    \n",
    "    # split into protected/non protected shapefiles and save them (not sure its necessary to save)\n",
    "    # selected protected shapes\n",
    "    # removes invalid geometries\n",
    "    p_sub = subzone_filter[subzone_filter[\"protected\"] == True]\n",
    "    p_sub_clean = p_sub[(p_sub.geometry.type == \"Polygon\") | (p_sub.geometry.type == \"MultiPolygon\")]\n",
    "    p_sub_clean = p_sub_clean.dissolve(by = \"szs\")\n",
    "\n",
    "    # select unprotected shapes\n",
    "    np_sub = subzone_filter[subzone_filter[\"protected\"] == False]\n",
    "    # removes invalid geometries\n",
    "    np_sub_clean = np_sub[(np_sub.geometry.type == \"Polygon\") | (np_sub.geometry.type == \"MultiPolygon\")]\n",
    "    np_sub_clean = np_sub_clean.dissolve(by = \"szs\")\n",
    "    \n",
    "    # save locations\n",
    "    p_raster_loc = os.path.join(scratch, \"p-\" + soi + \".tif\")\n",
    "    np_raster_loc = os.path.join(scratch, \"np-\" + soi + \".tif\")\n",
    "    \n",
    "    # generate masks from the polygons\n",
    "    p_mask = rasterize_polygon_mask(p_sub_clean, p_raster_loc, meta)\n",
    "    np_mask = rasterize_polygon_mask(np_sub_clean, np_raster_loc, meta)\n",
    "    \n",
    "    # this is where the forest mask would come in\n",
    "    # unless i do it in R?\n",
    "\n",
    "    p_pixels = np.sum(p_mask)\n",
    "    np_pixels = np.sum(np_mask) \n",
    "    \n",
    "    if p_pixels < np_pixels:\n",
    "        np_sampled = sample_raster(np_mask, np_raster_loc, np_pixels, p_pixels)\n",
    "        p_sampled = p_mask\n",
    "        \n",
    "        unsampled_loc = os.path.join(bec_mask_loc, \"p-\" + soi + \"-sampled.tif\")\n",
    "        \n",
    "        with rasterio.open(unsampled_loc, \"w+\", **meta) as out:\n",
    "            out.write_band(1, p_sampled)\n",
    "    \n",
    "    else:\n",
    "        p_sampled = sample_raster(p_mask, p_raster_loc, p_pixels, np_pixels)\n",
    "        np_sampled = np_mask\n",
    "        \n",
    "        unsampled_loc = os.path.join(bec_mask_loc, \"np-\" + soi + \"-sampled.tif\")\n",
    "        \n",
    "        with rasterio.open(unsampled_loc, \"w+\", **meta) as out:\n",
    "            out.write_band(1, np_sampled)\n",
    "            \n",
    "    return p_sampled, np_sampled\n",
    "\n",
    "def load_data(variable, year):\n",
    "    if variable in structure_vars:\n",
    "        file_loc = os.path.join(merged, variable, \"BC-\" + variable + \"-\" + str(year) + \".tif\")\n",
    "        \n",
    "    elif variable == \"Change_Attribution\" or variable == \"Greatest_Change_Year\":\n",
    "        file_loc = os.path.join(merged, variable, \"BC-\" + variable + \".tif\") \n",
    "        \n",
    "    elif variable == \"vlce\":\n",
    "        file_loc = os.path.join(merged, variable, \"BC-\" + variable + \"-\" + str(year) + \".tif\")\n",
    "        \n",
    "    elif variable in elev_vars:\n",
    "        file_loc = os.path.join(merged, \"elevation\", \"BC-\" + variable + \".tif\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Not a valid variable\")\n",
    "        return\n",
    "    \n",
    "    #print(file_loc)\n",
    "    \n",
    "    with rasterio.open(file_loc) as rst:\n",
    "        rst_data = rst.read(1)\n",
    "        \n",
    "    return rst_data\n",
    "\n",
    "def get_data(mask, in_data, year):\n",
    "        \n",
    "    indexes = np.where(mask == 1)\n",
    "    \n",
    "    data_arr = in_data[indexes[0], indexes[1]]\n",
    "    \n",
    "    if variable in structure_vars:\n",
    "        divisor = structure_key[structure_key[\"variable\"] == variable].iloc[0][\"divide_by\"]\n",
    "        #print(divisor)\n",
    "        data_arr = data_arr / divisor\n",
    "    \n",
    "    return data_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231b28f",
   "metadata": {},
   "source": [
    "1 min per variable-year-soi\n",
    "5 variables\n",
    "128 soi\n",
    "1 year\n",
    "10 hours\n",
    "\n",
    "35 years\n",
    "15 days\n",
    "\n",
    "\n",
    "should i be including topographic variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38164ed1",
   "metadata": {},
   "source": [
    "# New df method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2218d9",
   "metadata": {},
   "source": [
    "This is where all the data is loaded in, it kind of works as a raster stack that is then transformed into a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169ac2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude loaded 19.10013437271118\n"
     ]
    }
   ],
   "source": [
    "# generate latitude raster\n",
    "# get x,y dimensions, ymin (bottom), and pixel size\n",
    "# make a raster of equal size\n",
    "# where each cell is equal to its row.\n",
    "# then multiply by pixel size and add ymin\n",
    "start_time = time.time()\n",
    "raster_loc = os.path.join(\"D:\\\\\", \"Merged\", \"Change_Attribution\", \"BC-Change_Attribution.tif\")\n",
    "\n",
    "# load data and get bounds\n",
    "with rasterio.open(raster_loc) as rst:\n",
    "    data = rst.read(1)\n",
    "    bounds = rst.bounds\n",
    "    \n",
    "lat_min = bounds[1]\n",
    "x, y = data.shape\n",
    "an_array = np.array(np.flip(np.arange(0, x)))\n",
    "\n",
    "repeats_array = np.transpose([an_array] * y)\n",
    "\n",
    "latitude = repeats_array\n",
    "#latitude = (repeats_array * 30) + lat_min\n",
    "# going to do this in R (post)\n",
    "# i think the world isnt a fan of floats\n",
    "\n",
    "# clean up\n",
    "del data\n",
    "del bounds\n",
    "del repeats_array\n",
    "del an_array\n",
    "del lat_min\n",
    "print(\"latitude loaded\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeaaad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elev 46.30174803733826\n",
      "slope 57.76206064224243\n",
      "elev_cv 29.236483097076416\n",
      "loreys_height 31.074626207351685\n",
      "percentage_first_returns_above_2m 32.356269121170044\n",
      "total_biomass 33.50341033935547\n",
      "vlce 17.804847478866577\n",
      "Change_Attribution 4.043978691101074\n",
      "281.74160289764404\n"
     ]
    }
   ],
   "source": [
    "year = 2015\n",
    "\n",
    "arrays = {}\n",
    "start = time.time()\n",
    "for variable in all_vars:\n",
    "    mid = time.time()\n",
    "    arrays[variable] = load_data(variable, year)\n",
    "    print(variable, time.time() - mid)\n",
    "\n",
    "change_year = load_data(\"Greatest_Change_Year\", year)\n",
    "\n",
    "arrays[\"change_year\"] = change_year\n",
    "arrays[\"latitude\"] = latitude\n",
    "del change_year\n",
    "del latitude\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df44e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elev\n",
      "(45599, 53242)\n",
      "slope\n",
      "(45599, 53242)\n",
      "elev_cv\n",
      "(45598, 53241)\n",
      "loreys_height\n",
      "(45598, 53241)\n",
      "percentage_first_returns_above_2m\n",
      "(45598, 53241)\n",
      "total_biomass\n",
      "(45598, 53241)\n",
      "vlce\n",
      "(45598, 53241)\n",
      "Change_Attribution\n",
      "(45598, 53241)\n",
      "change_year\n",
      "(45598, 53241)\n",
      "latitude\n",
      "(45598, 53241)\n"
     ]
    }
   ],
   "source": [
    "for keys in arrays:\n",
    "    print(keys)\n",
    "    print(arrays[keys].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9145b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBPS_mk 2015\n",
      "making data arrays\n",
      "merging and saving\n",
      "144.36425971984863\n"
     ]
    }
   ],
   "source": [
    "# for the single subzone with no protected area. needs to be included in the BEC proportions figure\n",
    "# only one or i would solve in functions\n",
    "\n",
    "soi = \"SBPS_mk\"\n",
    "year = 2015\n",
    "\n",
    "start = time.time()\n",
    "print(soi, year)\n",
    "\n",
    "h_mask = rasterio.open(h_mask_loc)\n",
    "h_mask_data = h_mask.read(1)\n",
    "meta = h_mask.meta.copy()\n",
    "meta[\"nodata\"] = 0\n",
    "del h_mask\n",
    "del h_mask_data\n",
    "\n",
    "\n",
    "csv_save = soi + \"-\" + str(year) + \".csv\"\n",
    "save_location = os.path.join(\"..\", \"data\", \"all_vars\", csv_save)\n",
    "\n",
    "if not os.path.isfile(save_location):\n",
    "    \n",
    "    np.random.seed(69420)\n",
    "    \n",
    "    # filter ppa for the subzone of interest\n",
    "    subzone_filter = ppa_bec[ppa_bec[\"szs\"] == soi]\n",
    "    \n",
    "    # select unprotected shapes\n",
    "    np_sub = subzone_filter[subzone_filter[\"protected\"] == False]\n",
    "    # removes invalid geometries\n",
    "    np_sub_clean = np_sub[(np_sub.geometry.type == \"Polygon\") | (np_sub.geometry.type == \"MultiPolygon\")]\n",
    "    np_sub_clean = np_sub_clean.dissolve(by = \"szs\")\n",
    "    \n",
    "    # save locations\n",
    "    np_raster_loc = os.path.join(scratch, \"np-\" + soi + \".tif\")\n",
    "    \n",
    "    np_mask = rasterize_polygon_mask(np_sub_clean, np_raster_loc, meta)\n",
    "\n",
    "    print(\"making data arrays\")\n",
    "    #protected = []\n",
    "    unprotected = []\n",
    "    for key in arrays:\n",
    "        variable = key\n",
    "        #protected.append(get_data(masks[0], arrays[key], year))\n",
    "        unprotected.append(get_data(np_mask, arrays[key], year))\n",
    "\n",
    "    #df_p = pd.DataFrame(np.transpose(np.vstack(protected)), columns = all_vars + [\"change_year\"])\n",
    "    #df_p[\"protected\"] = \"protected\"\n",
    "\n",
    "    df_np = pd.DataFrame(np.transpose(np.vstack(unprotected)), columns = list(arrays.keys()))\n",
    "    df_np[\"protected\"] = \"unprotected\"\n",
    "\n",
    "    print(\"merging and saving\")\n",
    "\n",
    "    df = df_np\n",
    "    df[\"subzone\"] = soi\n",
    "    df[\"year\"] = year\n",
    "\n",
    "    df.to_csv(save_location, index = False)\n",
    "print(time.time() - start)\n",
    "        \n",
    "#del arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4af6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAFA_un 2015\n",
      "loading masks\n",
      "making data arrays\n",
      "elev\n",
      "18.518768787384033\n",
      "slope\n",
      "18.4835364818573\n",
      "elev_cv\n",
      "19.096954107284546\n",
      "loreys_height\n",
      "19.73732352256775\n",
      "percentage_first_returns_above_2m\n",
      "20.465655088424683\n",
      "total_biomass\n",
      "20.890056848526\n",
      "vlce\n",
      "19.866174936294556\n",
      "Change_Attribution\n",
      "19.887165069580078\n",
      "change_year\n",
      "19.238274812698364\n",
      "latitude\n",
      "20.16164469718933\n",
      "making dfs took 223.45403742790222\n",
      "merging and saving\n",
      "save time 520.6137571334839\n",
      "806.0054888725281\n",
      "\n",
      "BAFA_unp 2015\n",
      "loading masks\n",
      "making data arrays\n",
      "elev\n",
      "15.140332460403442\n",
      "slope\n",
      "15.03517484664917\n",
      "elev_cv\n",
      "14.824721813201904\n",
      "loreys_height\n",
      "14.724886894226074\n",
      "percentage_first_returns_above_2m\n",
      "14.660131216049194\n",
      "total_biomass\n",
      "14.686352014541626\n",
      "vlce\n",
      "14.744583368301392\n",
      "Change_Attribution\n",
      "15.07864785194397\n",
      "change_year\n",
      "14.757972478866577\n",
      "latitude\n",
      "15.15526795387268\n",
      "making dfs took 171.08597373962402\n",
      "merging and saving\n",
      "save time 12.62341570854187\n",
      "185.77631855010986\n",
      "\n",
      "BG_xh 2015\n",
      "loading masks\n",
      "making data arrays\n",
      "elev\n",
      "14.818664073944092\n",
      "slope\n",
      "14.70908784866333\n",
      "elev_cv\n",
      "14.843183040618896\n",
      "loreys_height\n",
      "15.109406471252441\n",
      "percentage_first_returns_above_2m\n",
      "14.761150598526001\n",
      "total_biomass\n",
      "14.87470531463623\n",
      "vlce\n",
      "15.01993465423584\n",
      "Change_Attribution\n",
      "14.916232347488403\n",
      "change_year\n",
      "15.12061882019043\n",
      "latitude\n",
      "15.870078325271606\n",
      "making dfs took 170.37735795974731\n",
      "merging and saving\n",
      "save time 9.829733848571777\n",
      "180.91978573799133\n",
      "\n",
      "BG_xw 2015\n",
      "loading masks\n",
      "making data arrays\n",
      "elev\n",
      "14.746151685714722\n",
      "slope\n",
      "14.732615947723389\n",
      "elev_cv\n",
      "14.664952278137207\n",
      "loreys_height\n",
      "14.55136251449585\n",
      "percentage_first_returns_above_2m\n",
      "14.956751346588135\n",
      "total_biomass\n",
      "14.820711135864258\n",
      "vlce\n",
      "15.013965845108032\n",
      "Change_Attribution\n",
      "14.781420230865479\n",
      "change_year\n",
      "14.898151874542236\n",
      "latitude\n",
      "14.83289885520935\n",
      "making dfs took 168.67642307281494\n",
      "merging and saving\n",
      "save time 9.098870754241943\n",
      "178.46955633163452\n",
      "\n",
      "BWBS_dk 2015\n",
      "loading masks\n",
      "making data arrays\n",
      "elev\n",
      "15.567352294921875\n",
      "slope\n",
      "15.689118146896362\n",
      "elev_cv\n",
      "15.470704555511475\n",
      "loreys_height\n",
      "15.286476373672485\n",
      "percentage_first_returns_above_2m\n",
      "15.498754024505615\n",
      "total_biomass\n",
      "15.348259925842285\n",
      "vlce\n",
      "15.48613429069519\n",
      "Change_Attribution\n",
      "15.091177701950073\n",
      "change_year\n",
      "15.436505317687988\n",
      "latitude\n"
     ]
    }
   ],
   "source": [
    "# using the whole sample from the masks, so not an equal sample when put into a csv\n",
    "\n",
    "for soi in subzones:\n",
    "    start = time.time()\n",
    "    print(soi, year)\n",
    "    csv_save = soi + \"-\" + str(year) + \".csv\"\n",
    "    save_location = os.path.join(\"..\", \"data\", \"all_vars\", csv_save)\n",
    "    \n",
    "    if not os.path.isfile(save_location):\n",
    "\n",
    "        print(\"loading masks\")\n",
    "        mask_locs = [os.path.join(scratch, protected + \"-\" + soi + \".tif\") for protected in [\"p\", \"np\"]]\n",
    "        mask_sum = sum([os.path.isfile(loc) for loc in mask_locs])\n",
    "\n",
    "        if mask_sum == 2:\n",
    "            masks = []\n",
    "\n",
    "            for i in range(len(mask_locs)):\n",
    "                with rasterio.open(mask_locs[i]) as rst:\n",
    "                    masks.append(rst.read(1))\n",
    "                    \n",
    "        else:\n",
    "            print(\"loading failed\")\n",
    "            print(\"generating new masks\")\n",
    "            masks = equal_sample_p_np(soi)\n",
    "        \n",
    "        print(\"making data arrays\")\n",
    "        protected = []\n",
    "        unprotected = []\n",
    "        for key in arrays:\n",
    "            \n",
    "            key_time = time.time()\n",
    "            variable = key\n",
    "            print(variable)\n",
    "            protected.append(get_data(masks[0], arrays[key], year))\n",
    "            unprotected.append(get_data(masks[1], arrays[key], year))\n",
    "            print(time.time() - key_time)\n",
    "            \n",
    "        df_p = pd.DataFrame(np.transpose(np.vstack(protected)), columns = list(arrays.keys()))\n",
    "        df_p[\"protected\"] = \"protected\"\n",
    "\n",
    "        df_np = pd.DataFrame(np.transpose(np.vstack(unprotected)), columns = list(arrays.keys()))\n",
    "        df_np[\"protected\"] = \"unprotected\"\n",
    "        \n",
    "        print(\"making dfs took\", time.time() - start)\n",
    "        print(\"merging and saving\")\n",
    "\n",
    "        df = pd.concat([df_p, df_np])\n",
    "        df[\"subzone\"] = soi\n",
    "        df[\"year\"] = year\n",
    "        \n",
    "        df[[\"vlce\", \"Change_Attribution\", \"change_year\", \"latitude\"]] = df[[\"vlce\", \"Change_Attribution\", \"change_year\", \"latitude\"]].apply(pd.to_numeric, downcast = \"integer\")\n",
    "        \n",
    "        save_time = time.time()\n",
    "        df.to_csv(save_location, index = False)\n",
    "        \n",
    "        print(\"save time\", time.time() - save_time)\n",
    "        \n",
    "    print(time.time() - start)\n",
    "    print()\n",
    "    #clear_output(wait = True)\n",
    "        \n",
    "del arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3c88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
